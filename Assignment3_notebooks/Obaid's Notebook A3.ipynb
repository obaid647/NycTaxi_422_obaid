{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ce86a2-fd34-419c-96ec-e1af7a145758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG nyc_taxi;\n",
    "USE SCHEMA nyc_taxi_schema;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e38b87-725a-4951-99d4-725795765d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS nyc_taxi.nyc_taxi_schema.sparkml_cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ac66b9-7f09-4f89-a35a-15bc2d898f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARKML_TEMP_DFS_PATH\"] = \"/Volumes/nyc_taxi/nyc_taxi_schema/sparkml_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875e914a-5f3c-4a1f-9ceb-ae1b7082fa13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "#delete any old models so Spark can free cache\n",
    "for name in [\"cvLr\", \"cvLrModel\", \"bestLrPipelineModel\", \"bestLrModel\",\n",
    "             \"rfModel\", \"bestRfPipelineModel\", \"bestRfModel\",\n",
    "             \"lrPipelineModel\", \"rfPipelineModel\", \"model\"]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f84c85-aa7b-4f70-81b9-0545a2dcf900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"nyc_taxi.nyc_taxi_schema.yellow_trips_csv_v\")\n",
    "df.createOrReplaceTempView(\"table\")\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7599d2a0-d86c-41c5-9f64-31213177479a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e03b8d7-2440-420f-a004-fd443a8bbe0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, unix_timestamp, hour, dayofweek\n",
    ")\n",
    "\n",
    "#base dataset without junk column\n",
    "df = df.drop(\"_rescued_data\")\n",
    "\n",
    "#create useful features\n",
    "dfReg = (\n",
    "    df\n",
    "    .withColumn(\"pickup_ts\",  unix_timestamp(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"dropoff_ts\", unix_timestamp(\"tpep_dropoff_datetime\"))\n",
    "    .withColumn(\"trip_duration_min\", (col(\"dropoff_ts\") - col(\"pickup_ts\")) / 60.0)\n",
    "    .withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"pickup_dow\", dayofweek(\"tpep_pickup_datetime\"))\n",
    ")\n",
    "\n",
    "#basic filtering\n",
    "dfReg = (\n",
    "    dfReg\n",
    "    .filter(col(\"fare_amount\") > 0)\n",
    "    .filter(col(\"trip_distance\") > 0)\n",
    "    .filter(col(\"trip_duration_min\") > 0)\n",
    "    .filter(col(\"passenger_count\") > 0)\n",
    ")\n",
    "\n",
    "numericFeatures = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"trip_duration_min\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_dow\",\n",
    "    \"extra\",\n",
    "    \"mta_tax\",\n",
    "    \"tip_amount\",\n",
    "    \"tolls_amount\",\n",
    "    \"improvement_surcharge\"\n",
    "]\n",
    "\n",
    "categoricalFeatures = [\n",
    "    \"VendorID\",\n",
    "    \"RateCodeID\",\n",
    "    \"store_and_fwd_flag\",\n",
    "    \"payment_type\"\n",
    "]\n",
    "\n",
    "#remove nulls for cols used as features + label\n",
    "colsToKeepNotNull = numericFeatures + categoricalFeatures + [\"fare_amount\"]\n",
    "\n",
    "dfReg = dfReg.na.drop(subset=colsToKeepNotNull)\n",
    "\n",
    "trainDf, testDf = dfReg.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "dfReg.printSchema()\n",
    "display(dfReg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2fd956-e387-4b8f-b77a-62ac2cb1c3f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#index categoricals\n",
    "indexers = [\n",
    "    StringIndexer(\n",
    "        inputCol=c,\n",
    "        outputCol=c + \"_idx\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    for c in categoricalFeatures\n",
    "]\n",
    "\n",
    "#one-hot encode\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[c + \"_idx\" for c in categoricalFeatures],\n",
    "    outputCols=[c + \"_oh\" for c in categoricalFeatures]\n",
    ")\n",
    "\n",
    "#assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numericFeatures + [c + \"_oh\" for c in categoricalFeatures],\n",
    "    outputCol=\"features_unscaled\"\n",
    ")\n",
    "\n",
    "#scale for linear regression\n",
    "#withMean=False keeps vector sparse to avoid huge memory usage\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_unscaled\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=False, #True\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "#linear regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"fare_amount\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "#full linear regression pipeline\n",
    "lrPipeline = Pipeline(\n",
    "    stages=indexers + [encoder, assembler, scaler, lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87555104-6fff-405e-9346-685cf5ab6241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#param grid\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.0, 0.1]) # [0.0, 0.01, 0.1]\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5]) # [0.0, 0.5, 1.0]\n",
    "    .addGrid(lr.maxIter, [50]) # [50, 100]\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluatorRmse = RegressionEvaluator(\n",
    "    labelCol=\"fare_amount\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "cvLr = CrossValidator(\n",
    "    estimator=lrPipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluatorRmse,\n",
    "    numFolds=2,     #lower if needed\n",
    "    parallelism=1,  #lower if needed\n",
    "    collectSubModels=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7ba3e1-43da-4036-be7f-0737bd69e7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cvLrModel = cvLr.fit(trainDf)\n",
    "\n",
    "bestLrPipelineModel = cvLrModel.bestModel\n",
    "bestLrModel = bestLrPipelineModel.stages[-1]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8380561237667516,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Obaid's Notebook A3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
