{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ce86a2-fd34-419c-96ec-e1af7a145758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG nyc_taxi;\n",
    "USE SCHEMA nyc_taxi_schema;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e38b87-725a-4951-99d4-725795765d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS nyc_taxi.nyc_taxi_schema.sparkml_cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ac66b9-7f09-4f89-a35a-15bc2d898f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARKML_TEMP_DFS_PATH\"] = \"/Volumes/nyc_taxi/nyc_taxi_schema/sparkml_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875e914a-5f3c-4a1f-9ceb-ae1b7082fa13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "#delete any old models so Spark can free cache\n",
    "for name in [\"cvLr\", \"cvLrModel\", \"bestLrPipelineModel\", \"bestLrModel\",\n",
    "             \"rfModel\", \"bestRfPipelineModel\", \"bestRfModel\",\n",
    "             \"lrPipelineModel\", \"rfPipelineModel\", \"model\"]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85f84c85-aa7b-4f70-81b9-0545a2dcf900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"nyc_taxi.nyc_taxi_schema.yellow_trips_csv_v\")\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7599d2a0-d86c-41c5-9f64-31213177479a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8304f7f0-9c85-4813-a685-8c995407ecc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import radians, sin, cos, atan2, sqrt\n",
    "\n",
    "R = 3959  #radius of earth in miles\n",
    "\n",
    "#filter original df\n",
    "dfReg = (\n",
    "    df\n",
    "    .filter((F.col(\"fare_amount\") > 0) &\n",
    "            (F.col(\"fare_amount\") < 500) &\n",
    "            (F.col(\"trip_distance\") > 0))\n",
    "    .withColumn(\"pickup_hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"pickup_dow\", F.dayofweek(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"pickup_ts\",  F.unix_timestamp(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"dropoff_ts\", F.unix_timestamp(\"tpep_dropoff_datetime\"))\n",
    "    .withColumn(\"trip_duration_min\",\n",
    "                (F.col(\"dropoff_ts\") - F.col(\"pickup_ts\")) / 60.0)\n",
    ")\n",
    "\n",
    "#remove bad durations and passenger counts\n",
    "dfReg = (\n",
    "    dfReg\n",
    "    .filter(F.col(\"trip_duration_min\") > 0)\n",
    "    .filter(F.col(\"passenger_count\") > 0)\n",
    ")\n",
    "\n",
    "#straight line distance\n",
    "dfReg = (\n",
    "    dfReg\n",
    "    .withColumn(\"pick_lat\",  radians(F.col(\"pickup_latitude\")))\n",
    "    .withColumn(\"pick_long\", radians(F.col(\"pickup_longitude\")))\n",
    "    .withColumn(\"drop_lat\",  radians(F.col(\"dropoff_latitude\")))\n",
    "    .withColumn(\"drop_long\", radians(F.col(\"dropoff_longitude\")))\n",
    ")\n",
    "\n",
    "dfReg = (\n",
    "    dfReg\n",
    "    .withColumn(\"lat_diff\",  F.col(\"pick_lat\") - F.col(\"drop_lat\"))\n",
    "    .withColumn(\"long_diff\", F.col(\"pick_long\") - F.col(\"drop_long\"))\n",
    ")\n",
    "\n",
    "dfReg = (\n",
    "    dfReg\n",
    "    .withColumn(\n",
    "        \"a\",\n",
    "        sin(F.col(\"lat_diff\") / 2) ** 2\n",
    "        + cos(F.col(\"pick_lat\")) * cos(F.col(\"drop_lat\"))\n",
    "        * sin(F.col(\"long_diff\") / 2) ** 2\n",
    "    )\n",
    "    .withColumn(\"c\", 2 * atan2(sqrt(F.col(\"a\")), sqrt(1 - F.col(\"a\"))))\n",
    ")\n",
    "\n",
    "dfReg = dfReg.withColumn(\n",
    "    \"straight_line_distance\",\n",
    "    F.col(\"c\") * F.lit(R)\n",
    ")\n",
    "\n",
    "#drop helper columns used for calculations\n",
    "dfReg = dfReg.drop(\n",
    "    \"pick_lat\", \"pick_long\", \"drop_lat\", \"drop_long\",\n",
    "    \"lat_diff\", \"long_diff\", \"a\", \"c\"\n",
    ")\n",
    "\n",
    "#features that cant be null\n",
    "featureCols = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"trip_duration_min\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_dow\",\n",
    "    \"straight_line_distance\"\n",
    "]\n",
    "\n",
    "labelCol = \"fare_amount\"\n",
    "\n",
    "#drop if rows are null\n",
    "dfReg = dfReg.na.drop(subset=featureCols)\n",
    "\n",
    "#numeric features list for the model\n",
    "numericFeatures = featureCols + [\"straight_line_distance\"]\n",
    "\n",
    "print(\"Rows after cleaning:\", dfReg.count())\n",
    "\n",
    "#train/test split\n",
    "trainDf, testDf = dfReg.randomSplit([0.7, 0.3], seed=42)\n",
    "print(\"Train rows:\", trainDf.count())\n",
    "print(\"Test rows:\", testDf.count())\n",
    "\n",
    "#cross validation sample. Instructor approved\n",
    "train_count = trainDf.count()\n",
    "fraction = min(1.0, 100000.0 / float(train_count))\n",
    "trainDfCv = trainDf.sample(withReplacement=False,\n",
    "                           fraction=fraction,\n",
    "                           seed=42)\n",
    "print(\"Rows used for CV:\", trainDfCv.count())\n",
    "\n",
    "display(dfReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ff0b0d-982e-4a17-a372-ef1757bff85a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pipeline 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2fd956-e387-4b8f-b77a-62ac2cb1c3f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numericFeatures,\n",
    "    outputCol=\"features_unscaled\"\n",
    ")\n",
    "\n",
    "#scale for linear regression\n",
    "#withMean=False keeps vector sparse to avoid huge memory usage\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_unscaled\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True, #True\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "#linear regression model\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=labelCol,\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "#full linear regression pipeline\n",
    "lrPipeline = Pipeline(\n",
    "    stages=[assembler, scaler, lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87555104-6fff-405e-9346-685cf5ab6241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#param grid\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.0, 0.0001, 0.001, 0.01])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5])\n",
    "    .addGrid(lr.maxIter, [50, 100])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "evaluatorRmse = RegressionEvaluator(\n",
    "    labelCol=labelCol,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "cvLr = CrossValidator(\n",
    "    estimator=lrPipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluatorRmse,\n",
    "    numFolds=3,     #lower if needed\n",
    "    parallelism=1  #lower if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7ba3e1-43da-4036-be7f-0737bd69e7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cvLrModel = cvLr.fit(trainDfCv)\n",
    "\n",
    "bestLrPipelineModel = cvLrModel.bestModel\n",
    "bestLrModel = bestLrPipelineModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e9d94f-4a9b-45db-8683-d6fb9d93751f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#predictions on test set\n",
    "testPredLr = bestLrPipelineModel.transform(testDf)\n",
    "\n",
    "#evaluation metrics\n",
    "rmseTest = evaluatorRmse.evaluate(testPredLr)\n",
    "evaluatorR2 = RegressionEvaluator(\n",
    "    labelCol=labelCol,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "r2Test = evaluatorR2.evaluate(testPredLr)\n",
    "\n",
    "print(\"Linear Regression Evaluation Metrics:\")\n",
    "print(\"RMSE:\", rmseTest)\n",
    "print(\"R2:\", r2Test)\n",
    "\n",
    "#predictions vs actual\n",
    "testPredLr.select(\"fare_amount\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b67967-bded-4707-b95f-ed2dec404c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best Linear Regression Hyperparameters\")\n",
    "print(\"regParam:\", bestLrModel.getRegParam())\n",
    "print(\"elasticNetParam:\", bestLrModel.getElasticNetParam())\n",
    "print(\"maxIter:\", bestLrModel.getMaxIter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd6cf9f-991f-43ea-b02f-416bc478713c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pipeline 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb844db-2b49-44d3-a4c7-4000a3049183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=labelCol,\n",
    "    predictionCol=\"prediction\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rfPipeline = Pipeline(\n",
    "    stages=[assembler, scaler, rf]\n",
    ")\n",
    "\n",
    "rfparamGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [50, 100])\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "rfCV = CrossValidator(\n",
    "    estimator=rfPipeline,\n",
    "    estimatorParamMaps=rfparamGrid,\n",
    "    evaluator=evaluatorRmse, #same RMSE evaluator\n",
    "    numFolds=3,     #lower if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dcab810-c1df-4694-9f2d-973cdc6351d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rfCvModel = rfCV.fit(trainDfCv)\n",
    "\n",
    "bestRfPipelineModel = rfCvModel.bestModel\n",
    "bestRfModel = bestRfPipelineModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bee58993-54e2-4a0c-8a16-7832cc344d8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#predictions on test set\n",
    "testPredRf = bestRfPipelineModel.transform(testDf)\n",
    "\n",
    "#evaluation metrics\n",
    "rmseTest = evaluatorRmse.evaluate(testPredRf)\n",
    "evaluatorR2 = RegressionEvaluator(\n",
    "    labelCol=labelCol,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "r2Test = evaluatorR2.evaluate(testPredRf)\n",
    "\n",
    "print(\"Linear Regression Evaluation Metrics:\")\n",
    "print(\"RMSE:\", rmseTest)\n",
    "print(\"R2:\", r2Test)\n",
    "\n",
    "#predictions vs actual\n",
    "testPredRf.select(\"fare_amount\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d2a1989-4a54-4ae0-b254-1e656fbefcb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best Random Forest Hyperparameters\")\n",
    "print(\"numTrees:\", bestRfModel.getNumTrees())\n",
    "print(\"maxDepth:\", bestRfModel.getMaxDepth())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8380561237667516,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Obaid's Notebook A3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
